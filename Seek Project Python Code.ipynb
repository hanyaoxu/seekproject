{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "external-equity",
   "metadata": {},
   "source": [
    "## Seek jobs Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "duplicate-latter",
   "metadata": {},
   "source": [
    "### data engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-party",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(file_name, content):\n",
    "    with open(file_name,'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "def read_file(file_name):\n",
    "    with open(file_name,'r',encoding='utf-8') as f:\n",
    "        return f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-teaching",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_page_url(page):\n",
    "    next_button_selector = page.select('a[data-automation=\"page-next\"]')\n",
    "    for item in next_button_selector:\n",
    "            return item.attrs['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-eclipse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_download(url,file_name):\n",
    "\n",
    "    if os.path.isfile(file_name):\n",
    "        print (f'{url} already exists as {file_name}')\n",
    "        return read_file(file_name)\n",
    "    else:\n",
    "        content = requests.get(url).text\n",
    "        write_file(file_name,content)\n",
    "        print (f'{url} downloaded to {file_name}')\n",
    "        return read_file(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(base_url,page_url,get_next_page_url):\n",
    "    html = requests.get(base_url + page_url).text\n",
    "    page = BeautifulSoup(html)\n",
    "    current_urls = set([base_url + page_url])\n",
    "    next_page_url = get_next_page_url(page)        \n",
    "    if next_page_url:\n",
    "        return current_urls | get_urls(base_url,next_page_url,get_next_page_url)\n",
    "    else:\n",
    "        return current_urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.seek.com.au'\n",
    "start_page = '/data-engineer-jobs/in-All-Sydney-NSW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "            for urls in get_urls(base_url,start_page,get_next_page_url):\n",
    "    print(urls)\n",
    "    html = requests.get(urls).text\n",
    "    page = BeautifulSoup(html)\n",
    "    for link in page.select('a[data-automation=\"jobTitle\"]'):\n",
    "        job_links = base_url + link.attrs['href']\n",
    "        job_id = re.findall(r'(\\d+)',job_links)[0]\n",
    "        print(job_id)\n",
    "        get_or_download(job_links,f'../seek project/data engineer jobs/{job_id}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-bottle",
   "metadata": {},
   "source": [
    "### data scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-appeal",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_page = '/data-scientist-jobs/in-All-Sydney-NSW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "for urls in get_urls(base_url,start_page,get_next_page_url):\n",
    "    print(urls)\n",
    "    html = requests.get(urls).text\n",
    "    page = BeautifulSoup(html)\n",
    "    for link in page.select('a[data-automation=\"jobTitle\"]'):\n",
    "        job_links = base_url + link.attrs['href']\n",
    "        job_id = re.findall(r'(\\d+)',job_links)[0]\n",
    "        print(job_id)\n",
    "        get_or_download(job_links,f'../seek project/data scientist jobs/{job_id}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-november",
   "metadata": {},
   "source": [
    "### data analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_page = '/data-analyst-jobs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "for urls in get_urls(base_url,start_page,get_next_page_url):\n",
    "    print(urls)\n",
    "    html = requests.get(urls).text\n",
    "    page = BeautifulSoup(html)\n",
    "    for link in page.select('a[data-automation=\"jobTitle\"]'):\n",
    "        job_links = base_url + link.attrs['href']\n",
    "        job_id = re.findall(r'(\\d+)',job_links)[0]\n",
    "        print(job_id)\n",
    "        get_or_download(job_links,f'../data analyst jobs/{job_id}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-authority",
   "metadata": {},
   "source": [
    "## Seek jobs Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(file_name, content):\n",
    "    with open(file_name,'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "\n",
    "def read_file(file_name):\n",
    "    with open(file_name,'r',encoding='utf-8') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_duties(file_location):\n",
    "    tabs = []\n",
    "    for file_name in glob.glob(file_location):\n",
    "        print(f'running:{file_name}')\n",
    "        job_id = int(re.findall(r'\\d+', file_name)[0])\n",
    "        page = BeautifulSoup(read_file(file_name))\n",
    "        try:\n",
    "            job_card = page.select('div[data-automation=\"job-detail-page\"]')\n",
    "            ul_cards = job_card[0].find_all('ul')\n",
    "            for ul_index,ul_item in enumerate(ul_cards):\n",
    "                for li_index,li_item in enumerate(ul_item.find_all('li')):\n",
    "                    tabs.append({\n",
    "                        'job_id':job_id,\n",
    "                        'description_index':ul_index,\n",
    "                        'job_duty_index':li_index,\n",
    "                        'job_duty':li_item.getText()\n",
    "                    })\n",
    "        except IndexError:\n",
    "                print('No Job Detail Page')\n",
    "    cols = ['job_id','description_index','job_duty_index','job_duty']\n",
    "    df = pd.DataFrame(tabs)[cols]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_titles(file_location):\n",
    "    tabs = []\n",
    "    for file_name in glob.glob(file_location):\n",
    "        print(f'running:{file_name}')\n",
    "        job_id = int(re.findall(r'\\d+', file_name)[0])\n",
    "        page = BeautifulSoup(read_file(file_name))\n",
    "        try: \n",
    "            title_card = page.select('h1[class=\"FYwKg C6ZIU_4 _3nVJR_4 _1H36Y_4 _2DNlq_4 _1NXQv_4\"]')[0]\n",
    "            title = title_card.getText()\n",
    "            tabs.append({\n",
    "                'job_id':job_id,\n",
    "                'job_title':title})\n",
    "        except IndexError:\n",
    "                print('No Job Detail Page') \n",
    "    cols = ['job_id','job_title']\n",
    "    df = pd.DataFrame(tabs)[cols]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupied-logic",
   "metadata": {},
   "source": [
    "### data analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_duties_df = get_job_duties('../data analyst jobs/*.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_duties_df.to_csv('../data/da_duties.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/da_duties.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://postgres:password@this_postgres')\n",
    "name = 'da_duties'\n",
    "df.to_sql(name, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_title_df = get_job_titles('../data analyst jobs/*.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_title_df.to_csv('../data/da_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/da_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://postgres:password@this_postgres')\n",
    "name = 'da_titles'\n",
    "df.to_sql(name, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-benjamin",
   "metadata": {},
   "source": [
    "### data engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_df = get_job_duties('../data engineer jobs/*.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_df.to_csv('../data/de_duties.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/de_duties.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://postgres:password@this_postgres')\n",
    "name = 'de_duties'\n",
    "df.to_sql(name, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_title_df = get_job_titles('../data engineer jobs/*.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_title_df.to_csv('../data/de_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/de_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://postgres:password@this_postgres')\n",
    "name = 'de_titles'\n",
    "df.to_sql(name, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-preservation",
   "metadata": {},
   "source": [
    "### data scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df = get_job_duties('../data scientist jobs/*.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df.to_csv('../data/ds_duties.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ds_duties.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://postgres:password@this_postgres')\n",
    "name = 'ds_duties'\n",
    "df.to_sql(name, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_title_df = get_job_titles('../data scientist jobs/*.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_title_df.to_csv('../data/ds_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/ds_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://postgres:password@this_postgres')\n",
    "name = 'ds_titles'\n",
    "df.to_sql(name, engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-symphony",
   "metadata": {},
   "source": [
    "## To Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-shame",
   "metadata": {},
   "source": [
    "### Data Analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as pg2\n",
    "import pandas as pd\n",
    "\n",
    "con = pg2.connect(host='this_postgres',\n",
    "                  user='postgres',\n",
    "                  password='password',\n",
    "                  database='postgres')\n",
    "con.autocommit = True\n",
    "cur = con.cursor()\n",
    "\n",
    "def select(sql):\n",
    "    return pd.read_sql(sql,con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "with temp as (select job_duty,\n",
    "                     job_title\n",
    "              from da_duties\n",
    "                       left join da_titles\n",
    "                                 on da_duties.job_id = da_titles.job_id\n",
    "              where job_duty is not null)\n",
    "select job_duty\n",
    "from temp\n",
    "where (job_title ilike '%data%' and job_title ilike '%analyst%')\n",
    "   or job_title ilike '%insight%'\n",
    "'''\n",
    "df = select(sql)\n",
    "write_file('../data/da_duties.txt' , '\\n'.join(df['job_duty']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-climate",
   "metadata": {},
   "source": [
    "### Data Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''select  job_duty\n",
    "from de_duties\n",
    "where job_duty is not null'''\n",
    "df = select(sql)\n",
    "write_file('../data/de_duties.txt' , '\\n'.join(df['job_duty']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-cameroon",
   "metadata": {},
   "source": [
    "### Data Scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separate-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''select  job_duty\n",
    "from ds_duties\n",
    "where job_duty is not null'''\n",
    "df = select(sql)\n",
    "write_file('../data/ds_duties.txt' , '\\n'.join(df['job_duty']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
